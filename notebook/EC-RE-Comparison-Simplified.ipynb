{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Cache - Redis Enterprise Comparison\n",
    "\n",
    "A notebook to create a memorybound EC vs RE comparison.\n",
    "\n",
    "Using single DB RE deployments compaired side by side to their equivalent EC deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.express as px\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Get top level path\n",
    "path_parent = os.path.dirname(os.getcwd())\n",
    "os.chdir(path_parent)\n",
    "toppath = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EC Raw Data\n",
    "### Transform Raw EC data into useable datatable\n",
    "All EC node types and associated meta data, all EC node type Reserved prices\n",
    "\n",
    "Combine into clean datasheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = toppath+\"/data/EC-Supported-Node-Types.csv\"\n",
    "df_ec_nodes = pd.read_csv(path, sep=\",\")\n",
    "df_ec_nodes.tail(2)\n",
    "\n",
    "# remove Data Tiering Nodes for this analysis\n",
    "df_ec_nodes = df_ec_nodes[(df_ec_nodes['Node Version']!='Memory Optimized Cache Nodes with data tiering - Current Generation')]\n",
    "df_ec_nodes.tail(1)\n",
    "\n",
    "# Reserved prices, 1 year and 3 year\n",
    "path = toppath+\"/data/Reserved-EC-Prices.csv\"\n",
    "df_ec_prices = pd.read_csv(path, sep=\",\")\n",
    "df_ec_prices.head(2)\n",
    "\n",
    "df_ec_prices_1year = df_ec_prices[(df_ec_prices['Reservation Years']=='1 year')]\n",
    "df_ec_prices_1year = df_ec_prices_1year.add_prefix('1 Year ')\n",
    "df_ec_prices_1year.tail(1)\n",
    "\n",
    "df_ec_prices_3year = df_ec_prices[(df_ec_prices['Reservation Years']=='3 year')]\n",
    "df_ec_prices_3year = df_ec_prices_3year.add_prefix('3 Year ')\n",
    "df_ec_prices_3year.tail(1)\n",
    "\n",
    "\n",
    "### Merge data tables\n",
    "###(note, not all EC instances have reserved prices, so there are some nan values)\n",
    "df_ec_nodes_prices = df_ec_nodes.merge(df_ec_prices_1year, how='left', left_on='Cache Node Type', right_on='1 Year Node type')\n",
    "df_ec_nodes_prices = df_ec_nodes_prices.merge(df_ec_prices_3year, how='left', left_on='Cache Node Type', right_on='3 Year Node type')\n",
    "\n",
    "#### Clean datatable to usable format\n",
    "df_ec_nodes_prices.rename(columns = {'Cache Node Type':'EC Cache Node Type',\n",
    "                     'EC reserved memory percent (25%)':'Max Memory per Node (25% EC Reduction)',\n",
    "                     'vCPU':'EC vCPU',\n",
    "                     'Memory (GiB)':'EC RAM (GB)',\n",
    "                    'Network Performance':'EC Network (GiB)',\n",
    "                    'Price Per Hour (On-Demand) (US East (Ohio))':'EC On-Demand Cost/Hour/Node',\n",
    "                    '1 Year RI effective hourly rate**':'EC 1 Year RI effective hourly rate/Node',\n",
    "                    '3 Year RI effective hourly rate**':'EC 3 Year RI effective hourly rate/Node'},\n",
    "          inplace = True)\n",
    "\n",
    "# remove unnessesary columns\n",
    "df_ec_nodes_prices = df_ec_nodes_prices[['EC Cache Node Type',\n",
    "                    'Max Memory per Node (25% EC Reduction)',\n",
    "                    'EC vCPU',\n",
    "                    'EC RAM (GB)',\n",
    "                   'EC Network (GiB)',\n",
    "                    'EC On-Demand Cost/Hour/Node',\n",
    "                   'EC 1 Year RI effective hourly rate/Node',\n",
    "                   'EC 3 Year RI effective hourly rate/Node']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create EC node deployment datatable\n",
    "\n",
    "EC has 46 nodes but multiple deployment types:\n",
    "* 1 master no HA, \n",
    "* 1 master with HA (1-5 extra nodes)\n",
    "* Cluster mode:\n",
    "** multiple masters (up to 500 total nodes (including replicas) \n",
    "\n",
    "so each individual node can be deployed from a single master with no HA to a 500 node cluster with no HA, to a 250 master node, 250 replica node set up, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add additional columns\n",
    "* HA (Yes/No)\n",
    "* Cluster Mode (Yes/No)\n",
    "* Master Nodes Count\n",
    "* HA Nodes Count\n",
    "* Total Nodes (Master + HA)\n",
    "     * calc: (master nodes count * (HA nodes count + 1))\n",
    "* Total Dataset Size Possible (GB) (Master Nodes)\n",
    "* Total Memory Limit Possible (GB) (including replication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = toppath+\"/data/EC-Deployment-Options.csv\"\n",
    "df_ec_deployment = pd.read_csv(path, sep=\",\")\n",
    "df_ec_deployment.head()\n",
    "\n",
    "ec_node_type_list = df_ec_nodes_prices['EC Cache Node Type'].tolist()\n",
    "ec_node_type_list[0:3]\n",
    "\n",
    "# create new df, duplicate each ec deployment node options to each individual node type.\n",
    "# there are 1223 node deployment options per node type, there are 43 node types. \n",
    "# so a total of 56303 deployment options in EC\n",
    "\n",
    "df_ec_node_deployment_options = pd.DataFrame()\n",
    "for i in ec_node_type_list:\n",
    "    # each node type\n",
    "    print(i)\n",
    "    # add each node type to the deployment options\n",
    "    df_ec_deployment_1 = df_ec_deployment.copy()\n",
    "    df_ec_deployment_1['EC Cache Node Type'] = i\n",
    "    df_ec_deployment_1.reset_index(inplace=True)\n",
    "    df_ec_deployment_1 = df_ec_deployment_1.rename(columns = {'index':'id'})\n",
    "    # append each to a new df\n",
    "    df_ec_node_deployment_options = pd.concat([df_ec_node_deployment_options, df_ec_deployment_1])\n",
    "    print(df_ec_node_deployment_options.shape)\n",
    "    \n",
    "    \n",
    "df_ec_node_deployment_options['EC Cache Node Type (id)'] = df_ec_node_deployment_options['EC Cache Node Type'] + \" \" + df_ec_node_deployment_options['id'].astype(str)\n",
    "df_ec_node_deployment_options.tail()\n",
    "\n",
    "\n",
    "#### Combine the EC node deployment options to the EC node metadata\n",
    "df_ec_node_dply = df_ec_nodes_prices.merge(df_ec_node_deployment_options, how='left', left_on='EC Cache Node Type', right_on='EC Cache Node Type')\n",
    "\n",
    "### Perform calculations on merged data\n",
    "####Now that we have merged the EC node data to the EC deployment options metadata we can calculate deployment size and cost information.\n",
    "df_ec_node_dply[\"Total Dataset Size Possible (GB) (Master Nodes)\"] = df_ec_node_dply[\"Master Nodes Count\"] * df_ec_node_dply[\"Max Memory per Node (25% EC Reduction)\"]\n",
    "\n",
    "df_ec_node_dply[\"Total Memory Limit Possible (GB) (Including Replication)\"] = df_ec_node_dply[\"Total Nodes (Master + Replica)\"] * df_ec_node_dply[\"Max Memory per Node (25% EC Reduction)\"]\n",
    "\n",
    "df_ec_node_dply[\"Total On Demand Cost/Hour (All Nodes)\"] = df_ec_node_dply[\"EC On-Demand Cost/Hour/Node\"] * df_ec_node_dply[\"Total Nodes (Master + Replica)\"]\n",
    "\n",
    "df_ec_node_dply[\"Total On Demand Cost/Month (All Nodes)\"] = df_ec_node_dply[\"Total On Demand Cost/Hour (All Nodes)\"] * 730\n",
    "\n",
    "df_ec_node_dply[\"Total EC 1 RI Cost/Hour (All Nodes)\"] = df_ec_node_dply[\"EC 1 Year RI effective hourly rate/Node\"] * df_ec_node_dply[\"Total Nodes (Master + Replica)\"]\n",
    "\n",
    "df_ec_node_dply[\"Total EC 1 RI Cost/Month (All Nodes)\"] = df_ec_node_dply[\"Total EC 1 RI Cost/Hour (All Nodes)\"] * 730\n",
    "\n",
    "df_ec_node_dply[\"Total EC 3 RI Cost/Hour (All Nodes)\"] = df_ec_node_dply[\"EC 3 Year RI effective hourly rate/Node\"] * df_ec_node_dply[\"Total Nodes (Master + Replica)\"]\n",
    "\n",
    "df_ec_node_dply[\"Total EC 3 RI Cost/Month (All Nodes)\"] = df_ec_node_dply[\"Total EC 3 RI Cost/Hour (All Nodes)\"] * 730\n",
    "\n",
    "\n",
    "df_ec_node_dply.tail()\n",
    "\n",
    "\n",
    "path = toppath+\"/data/df_ec_node_deployment_options.csv\"\n",
    "df_ec_node_dply.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redis Enterprise Flexible Plan\n",
    "\n",
    "Use RE Flex Plan Datasheets for growing Db datasets.\n",
    "\n",
    "Large and High Throughput Shard types with and without HA.\n",
    "\n",
    "RE data set size Maxes out at 6250 for large shards and 625 for High throughput.\n",
    "\n",
    "Why? Because that is 500 shards. \n",
    "\n",
    "To my understanding going beyond a 500 shard RE database is tricky territory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = toppath+\"/data/RE Flex Deployment.xlsx\"\n",
    "df_re_large = pd.read_excel(path, sheet_name='Flex Large')\n",
    "df_re_large.tail(2)\n",
    "\n",
    "path = toppath+\"/data/RE Flex Deployment.xlsx\"\n",
    "df_re_ht = pd.read_excel(path, sheet_name='Flex High Throughput')\n",
    "df_re_ht.tail(2)\n",
    "\n",
    "# Concat Large shard deployments and High throughput shard deployment options together\n",
    "df_re = pd.concat([df_re_large,df_re_ht])\n",
    "print(df_re.shape)\n",
    "df_re.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it All Together (Merging EC and RE)\n",
    "\n",
    "Merging the tables together to get an acurate picture of RE vs EC means we need to examine each EC node deployment option (56304 unique options) to each individual RE Flex option (1500 unique options).\n",
    "\n",
    "This can be reduced though, we only need to look at HA vs HA and non-HA vs non-HA.\n",
    "\n",
    "It can be further reduced once the initial merge is done per RE Flex option to filter the options to within a percentage of total dataset size in the plus and minus direction.\n",
    "* (ie. keep only something like (-20% - +20%) RE total dataset size to EC total dataset size ). \n",
    "    * This is because we want to try and keep it apples to apples in some respect. Not compare a 25GB deployment to a 1TB deployment which doesnt make sense.\n",
    "\n",
    "A further reduction can be done to keep master shard counts relativly similar.\n",
    "* (ie. RE deployment has 10 master nodes, EC should have between 7 and 13 master nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we do this:\n",
    "\n",
    "First filter both dfs to be HA vs non HA tables.\n",
    "\n",
    "Then:\n",
    "\n",
    "EC-Node-DF on the left side, add a single row from the RE-df table \n",
    "* (ie. a single RE flex deployment).\n",
    "\n",
    "Then peform calculations on datasize comparison between EC vs RE.\n",
    "\n",
    "Filter to within some pecent range (ie. +/-20%) of total dataset size.\n",
    "\n",
    "Then repeate the step and append to the final datatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE Datatable\n",
    "print(df_re.shape)\n",
    "df_re.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EC datatable\n",
    "print(df_ec_node_dply.shape)\n",
    "df_ec_node_dply.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Inputs\n",
    "#### Choose the Dataset size range you are interested in viewing\n",
    "#### Choose the range of EC deployment sizes to compare too\n",
    "#### Choose number of HA nodes to include\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### User inputs for RE vs EC comparison export\n",
    "\n",
    "### pick your dataset gb range\n",
    "min_RE_dataset_size_GB = 0\n",
    "max_RE_dataset_size_GB = 3000\n",
    "\n",
    "\n",
    "# you will want a range of total dataset GB size change.\n",
    "# this will allow you to view EC deployments within a range of your RE deployment\n",
    "# example: RE deployment dataset size = 100, view 15% +/- range and see all EC deployments between 75GB & 115GB\n",
    "###### !!!!! (keep this relatively small, otherwise the combo sheet could be millions of rows)\n",
    "min_percent_total_dataset_gb_change = -0.15\n",
    "max_percent_total_dataset_gb_change = 0.15\n",
    "\n",
    "# Same as above but with cluster size (master shard count range)\n",
    "# You may want to compare similar deployments, \n",
    "### no need to look at a RE deployment of 4 master shards compared to a EC deployment of 500 masters or vice versa\n",
    "### the percent differences may be more confusing here though.\n",
    "### example: EC deployment 10 masters: \n",
    "##### min shard count percent change of -80% means EC of 10 should only be compared to RE deployments of at minimum 2 masters\n",
    "##### max shard count percent change of 100 means, 1000% increase, \n",
    "##### so 10 EC shard deployment could be compared to an RE deployment of 1000 shards is 40 shards.\n",
    "\n",
    "# if you do not want to limit here, just put -1 as min, and 1000 as max. then it wont filter on anything.\n",
    "min_percent_total_master_shard_count_change = -0.8\n",
    "max_percent_total_master_shard_count_change = 100\n",
    "\n",
    "\n",
    "# min HA nodes must be >= 1, max can be up to 5\n",
    "# keeping the max to 2 reduces the datasheet size. making it more usable.\n",
    "# if you have over 2 HA nodes RE is most likley always cheaper anyway\n",
    "min_HA_Nodes_Count = 1\n",
    "max_HA_Nodes_Count = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run The Comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tables to Non-HA\n",
    "print(df_re.shape)\n",
    "\n",
    "df_re_noHA = df_re[(df_re['HA (Yes/No)'])=='No']\n",
    "df_re_noHA = df_re_noHA[(df_re_noHA['Shard Type'])=='Large']\n",
    "#df_re_noHA = df_re_noHA[(df_re_noHA['Dataset Size (GB)'])<=3000]\n",
    "df_re_noHA = df_re_noHA[(df_re_noHA['Dataset Size (GB)']>=min_RE_dataset_size_GB) & (df_re_noHA['Dataset Size (GB)']<=max_RE_dataset_size_GB)]\n",
    "print(df_re_noHA.shape)\n",
    "#df_re_noHA.tail(2)\n",
    "\n",
    "print(df_ec_node_dply.shape)\n",
    "df_ec_node_dply_noHA = df_ec_node_dply[(df_ec_node_dply['HA Nodes Count']==0)]\n",
    "#df_ec_node_dply_noHA = df_ec_node_dply_noHA[(df_ec_node_dply_noHA['Total Dataset Size Possible (GB) (Master Nodes)']<=3500)]\n",
    "print(df_ec_node_dply_noHA.shape)\n",
    "#df_ec_node_dply_noHA.tail(2)\n",
    "\n",
    "### NO HA Table\n",
    "df_re_noHA.reset_index()\n",
    "df_re_noHA = df_re_noHA.add_prefix('RE-')\n",
    "#df_re_noHA\n",
    "\n",
    "df_ec_node_dply_noHA1 = df_ec_node_dply_noHA\n",
    "print(df_ec_node_dply_noHA1.shape)\n",
    "df_ec_node_dply_noHA1.head(1)\n",
    "\n",
    "\n",
    "# iterate through rows:\n",
    "df_all_NoHA = pd.DataFrame()\n",
    "for index, row in df_re_noHA.iterrows():\n",
    "    #print(row)\n",
    "    #print('**********')\n",
    "    \n",
    "    count = -1\n",
    "    #df_all = pd.DataFrame()\n",
    "    #print(df_ec_node_dply_noHA1.shape)\n",
    "    df_ec_node_dply_noHA = df_ec_node_dply_noHA1.copy()\n",
    "    for i in row:\n",
    "        count +=1\n",
    "        #print(row.index[count])\n",
    "        #print(i)\n",
    "        df_ec_node_dply_noHA[row.index[count]] = i\n",
    "    #print(df_ec_node_dply_noHA.shape)    \n",
    "    df_ec_node_dply_noHA[\"Total Deployment Size Delta (GB) (RE - EC)\"] = df_ec_node_dply_noHA[\"RE-Total Dataset Size (Including Replication) (GB)\"] - df_ec_node_dply_noHA['Total Memory Limit Possible (GB) (Including Replication)']\n",
    "\n",
    "    df_ec_node_dply_noHA[\"% Deployment size Change (RE vs EC)\"] = (df_ec_node_dply_noHA['RE-Total Dataset Size (Including Replication) (GB)'] - df_ec_node_dply_noHA['Total Memory Limit Possible (GB) (Including Replication)'])/abs(df_ec_node_dply_noHA['Total Memory Limit Possible (GB) (Including Replication)'])\n",
    "\n",
    "    df_ec_node_dply_noHA[\"Total Master shard Size Delta (GB) (RE - EC)\"] = df_ec_node_dply_noHA['RE-Dataset Size (GB)'] - df_ec_node_dply_noHA['Total Dataset Size Possible (GB) (Master Nodes)']\n",
    "\n",
    "    df_ec_node_dply_noHA[\"% Total Master Shard Size Change (RE vs EC)\"] = (df_ec_node_dply_noHA['RE-Dataset Size (GB)'] - df_ec_node_dply_noHA['Total Dataset Size Possible (GB) (Master Nodes)'])/abs(df_ec_node_dply_noHA['Total Dataset Size Possible (GB) (Master Nodes)'])\n",
    "\n",
    "    df_ec_node_dply_noHA[\"ON DEMAND Price Delta (RE - EC)\"] = df_ec_node_dply_noHA['RE-Cost/Month'] - df_ec_node_dply_noHA['Total On Demand Cost/Month (All Nodes)']\n",
    "\n",
    "    df_ec_node_dply_noHA[\"ON DEMAND % Change (RE vs EC)\"] = (df_ec_node_dply_noHA['RE-Cost/Month'] - df_ec_node_dply_noHA['Total On Demand Cost/Month (All Nodes)'])/abs(df_ec_node_dply_noHA['Total On Demand Cost/Month (All Nodes)'])\n",
    "\n",
    "    df_ec_node_dply_noHA[\"Total Shard Count Delta (RE - EC)\"] = df_ec_node_dply_noHA['RE-Total Shard Count'] - df_ec_node_dply_noHA['Total Nodes (Master + Replica)']\n",
    "\n",
    "    df_ec_node_dply_noHA[\"% Total Shard Count Change (RE - EC)\"] = (df_ec_node_dply_noHA['RE-Total Shard Count'] - df_ec_node_dply_noHA['Total Nodes (Master + Replica)'])/abs(df_ec_node_dply_noHA['Total Nodes (Master + Replica)'])\n",
    "\n",
    "    df_ec_node_dply_noHA[\"Total Master Shard Count Delta (RE - EC)\"] = df_ec_node_dply_noHA['RE-Master Shard Count'] - df_ec_node_dply_noHA['Master Nodes Count']\n",
    "\n",
    "    df_ec_node_dply_noHA[\"% Total Master Shard Count Change (RE - EC)\"] = (df_ec_node_dply_noHA['RE-Master Shard Count'] - df_ec_node_dply_noHA['Master Nodes Count'])/abs(df_ec_node_dply_noHA['Master Nodes Count'])    \n",
    "    \n",
    "    #print(df_ec_node_dply_noHA.shape)\n",
    "    # Filter\n",
    "    df_ec_node_dply_noHA = df_ec_node_dply_noHA[(df_ec_node_dply_noHA['% Total Master Shard Size Change (RE vs EC)']>=min_percent_total_dataset_gb_change) & (df_ec_node_dply_noHA['% Total Master Shard Size Change (RE vs EC)']<=max_percent_total_dataset_gb_change)]\n",
    "    df_ec_node_dply_noHA = df_ec_node_dply_noHA[(df_ec_node_dply_noHA['% Total Master Shard Count Change (RE - EC)']>=min_percent_total_master_shard_count_change) & (df_ec_node_dply_noHA['% Total Master Shard Count Change (RE - EC)']<=max_percent_total_master_shard_count_change)]\n",
    "\n",
    "    #print(df_ec_node_dply_noHA.shape)\n",
    "    print(\"***\")\n",
    "    #df_all_NoHA = df_all_NoHA.append(df_ec_node_dply_noHA)\n",
    "    df_all_NoHA = pd.concat([df_all_NoHA, df_ec_node_dply_noHA])\n",
    "    print(df_all_NoHA.shape)\n",
    "    \n",
    "print(df_ec_node_dply_noHA1.shape)\n",
    "print(df_all_NoHA.shape)\n",
    "#df_all_NoHA.head(2)\n",
    "\n",
    "\n",
    "########## HA Nodes Comparison\n",
    "\n",
    "# Filter tables to Non-HA\n",
    "print(df_re.shape)\n",
    "\n",
    "df_re_HA = df_re[(df_re['HA (Yes/No)'])=='Yes']\n",
    "df_re_HA = df_re_HA[(df_re_HA['Shard Type'])=='Large']\n",
    "df_re_HA = df_re_HA[(df_re_HA['Dataset Size (GB)']>=min_RE_dataset_size_GB) & (df_re_HA['Dataset Size (GB)']<=max_RE_dataset_size_GB)]\n",
    "print(df_re_HA.shape)\n",
    "df_re_HA.head(2)\n",
    "\n",
    "print(df_ec_node_dply.shape)\n",
    "df_ec_node_dply_HA = df_ec_node_dply[(df_ec_node_dply['HA Nodes Count']>=min_HA_Nodes_Count) & (df_ec_node_dply['HA Nodes Count']<=max_HA_Nodes_Count)]\n",
    "\n",
    "print(df_ec_node_dply_HA.shape)\n",
    "df_ec_node_dply_HA.tail(2)\n",
    "\n",
    "df_re_HA.reset_index()\n",
    "df_re_HA = df_re_HA.add_prefix('RE-')\n",
    "df_re_HA.head(1)\n",
    "\n",
    "df_ec_node_dply_HA1 = df_ec_node_dply_HA\n",
    "print(df_ec_node_dply_HA1.shape)\n",
    "#df_ec_node_dply_HA1.head(1)\n",
    "\n",
    "\n",
    "# iterate through rows:\n",
    "df_all_HA = pd.DataFrame()\n",
    "for index, row in df_re_HA.iterrows():\n",
    "    #print(row)\n",
    "    #print('**********')\n",
    "    \n",
    "    count = -1\n",
    "    #df_all = pd.DataFrame()\n",
    "    #print(df_ec_node_dply_HA1.shape)\n",
    "    df_ec_node_dply_HA = df_ec_node_dply_HA1.copy()\n",
    "    for i in row:\n",
    "        count +=1\n",
    "        #print(row.index[count])\n",
    "        #print(i)\n",
    "        df_ec_node_dply_HA[row.index[count]] = i\n",
    "    #print(df_ec_node_dply_HA.shape)    \n",
    "    df_ec_node_dply_HA[\"Total Deployment Size Delta (GB) (RE - EC)\"] = df_ec_node_dply_HA[\"RE-Total Dataset Size (Including Replication) (GB)\"] - df_ec_node_dply_HA['Total Memory Limit Possible (GB) (Including Replication)']\n",
    "\n",
    "    df_ec_node_dply_HA[\"% Deployment size Change (RE vs EC)\"] = (df_ec_node_dply_HA['RE-Total Dataset Size (Including Replication) (GB)'] - df_ec_node_dply_HA['Total Memory Limit Possible (GB) (Including Replication)'])/abs(df_ec_node_dply_HA['Total Memory Limit Possible (GB) (Including Replication)'])\n",
    "\n",
    "    df_ec_node_dply_HA[\"Total Master shard Size Delta (GB) (RE - EC)\"] = df_ec_node_dply_HA['RE-Dataset Size (GB)'] - df_ec_node_dply_HA['Total Dataset Size Possible (GB) (Master Nodes)']\n",
    "\n",
    "    df_ec_node_dply_HA[\"% Total Master Shard Size Change (RE vs EC)\"] = (df_ec_node_dply_HA['RE-Dataset Size (GB)'] - df_ec_node_dply_HA['Total Dataset Size Possible (GB) (Master Nodes)'])/abs(df_ec_node_dply_HA['Total Dataset Size Possible (GB) (Master Nodes)'])\n",
    "\n",
    "    df_ec_node_dply_HA[\"ON DEMAND Price Delta (RE - EC)\"] = df_ec_node_dply_HA['RE-Cost/Month'] - df_ec_node_dply_HA['Total On Demand Cost/Month (All Nodes)']\n",
    "\n",
    "    df_ec_node_dply_HA[\"ON DEMAND % Change (RE vs EC)\"] = (df_ec_node_dply_HA['RE-Cost/Month'] - df_ec_node_dply_HA['Total On Demand Cost/Month (All Nodes)'])/abs(df_ec_node_dply_HA['Total On Demand Cost/Month (All Nodes)'])\n",
    "\n",
    "    df_ec_node_dply_HA[\"Total Shard Count Delta (RE - EC)\"] = df_ec_node_dply_HA['RE-Total Shard Count'] - df_ec_node_dply_HA['Total Nodes (Master + Replica)']\n",
    "\n",
    "    df_ec_node_dply_HA[\"% Total Shard Count Change (RE - EC)\"] = (df_ec_node_dply_HA['RE-Total Shard Count'] - df_ec_node_dply_HA['Total Nodes (Master + Replica)'])/abs(df_ec_node_dply_HA['Total Nodes (Master + Replica)'])\n",
    "\n",
    "    df_ec_node_dply_HA[\"Total Master Shard Count Delta (RE - EC)\"] = df_ec_node_dply_HA['RE-Master Shard Count'] - df_ec_node_dply_HA['Master Nodes Count']\n",
    "\n",
    "    df_ec_node_dply_HA[\"% Total Master Shard Count Change (RE - EC)\"] = (df_ec_node_dply_HA['RE-Master Shard Count'] - df_ec_node_dply_HA['Master Nodes Count'])/abs(df_ec_node_dply_HA['Master Nodes Count'])\n",
    "    #print(df_ec_node_dply_HA.shape)\n",
    "    # Filter\n",
    "    df_ec_node_dply_HA = df_ec_node_dply_HA[(df_ec_node_dply_HA['% Total Master Shard Size Change (RE vs EC)']>=min_percent_total_dataset_gb_change) & (df_ec_node_dply_HA['% Total Master Shard Size Change (RE vs EC)']<=max_percent_total_dataset_gb_change)]\n",
    "    df_ec_node_dply_HA = df_ec_node_dply_HA[(df_ec_node_dply_HA['% Total Master Shard Count Change (RE - EC)']>=min_percent_total_master_shard_count_change) & (df_ec_node_dply_HA['% Total Master Shard Count Change (RE - EC)']<=max_percent_total_master_shard_count_change)]\n",
    "    \n",
    "    #print(df_ec_node_dply_HA.shape)\n",
    "    print(\"***\")\n",
    "    #df_all_HA = df_all_HA.append(df_ec_node_dply_HA)\n",
    "    df_all_HA = pd.concat([df_all_HA,df_ec_node_dply_HA])\n",
    "    print(df_all_HA.shape)\n",
    "    \n",
    "    \n",
    "print(df_all_HA.shape)\n",
    "df_all_HA.head(2)\n",
    "\n",
    "\n",
    "######## Combine them together\n",
    "#df_all_combo = df_all_NoHA.append(df_all_HA)\n",
    "df_all_combo = pd.concat([df_all_NoHA,df_all_HA])\n",
    "print(df_all_combo.shape)\n",
    "\n",
    "#df_all_combo.head(1)\n",
    "df_all_combo.drop(['id','EC Cache Node Type (id)',\n",
    "                   'RE-Min Cluster Cost/Hour',\n",
    "                   'RE-Shard Cost/Hour',\n",
    "                   'EC On-Demand Cost/Hour/Node',\n",
    "                   'EC 1 Year RI effective hourly rate/Node',\n",
    "                   'EC 3 Year RI effective hourly rate/Node',\n",
    "                  ], axis=1)\n",
    "df_all_combo.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = toppath+\"/data/RE-EC-comparison-export.csv\"\n",
    "df_all_combo.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT STEP\n",
    "\n",
    "find all EC deployments larger than RE that are also cheaper than RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_all_combo1 = df_all_combo[(df_all_combo['ON DEMAND Price Delta (RE - EC)']>=0)]\n",
    "df_all_combo1 = df_all_combo1[(df_all_combo1['Total Master shard Size Delta (GB) (RE - EC)']<=0)]\n",
    "print(df_all_combo1.shape)\n",
    "df_all_combo1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = toppath+\"/data/RE-EC-comparison-export-EC-Bigger-and-Cheaper.csv\"\n",
    "df_all_combo1.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
